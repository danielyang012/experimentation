# -*- coding: utf-8 -*-
"""DY Commerce Stats_Engine.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_FOtfI3Nqm-_jvXwmtlkK1lafdInzhMm
"""

# Commented out IPython magic to ensure Python compatibility.
# @title Load Packages
# %pip install sqldf
# %pip install statsmodels
import sqldf
import pandas as pd
import seaborn as sns
from scipy import stats
import numpy as np
from google.cloud import bigquery
import random as r
import warnings
import matplotlib.pyplot as plt
import datetime, pytz
import os
import math
import time
from datetime import date, datetime, timedelta
from tqdm import tqdm
from scipy.stats import norm, binom
import seaborn as sns
from statsmodels.stats import proportion as prop
from matplotlib.ticker import FuncFormatter
import shutil

warnings.filterwarnings('ignore')

from google.colab import auth
auth.authenticate_user()
print('Authenticated')

# %load_ext google.colab.data_table

#@title Code to Refresh Directory
delete_directory_path = '/content/Experiment Analysis Outputs: HP Anchor Hero CTA to Plan Picker' # @param {type:"string"}
if os.path.exists(delete_directory_path):
    shutil.rmtree(delete_directory_path)  # This deletes a folder and its contents recursively
    print(f"Deleted {delete_directory_path}")
else:
    print(f"The folder '{delete_directory_path}' does not exist.")



# @title Experiment Parameters { run: "auto", vertical-output: true, form-width: "50%", display-mode: "both" }
id_type = "visitor id" # @param ["visitor id", "tracking id"]
experiment_id = 25003380671 # @param {type:"string"}
significance_level = .1 # @param {type:"number"}
experiment_start_date = '2023-08-09' # @param{type:"date"}
experiment_end_date = '2023-08-29'# @param{type:"date"}

url_1_metric_name = 'Visits' #@param {type:"string"}
url_2_metric_name = 'Clicks' #@param {type:"string"}
url_3_metric_name = 'Conversions' #@param {type:"string"}
url_4_metric_name = 'None' #@param {type:"string"}

bq_config_vars = {'project_id': 'nbcu-ds-sandbox-a-001',
                  'dataset_id': 'nbcu-ds-prod-001'}

which_url_determines_conversion = 3 # @param [3, 4]
which_url_determines_denominator = 1 # @param [1, 2,3]
control_name = 'Control' #@param {type:"string"}

url_metric_name_list = [url_1_metric_name,
            url_2_metric_name,
            url_3_metric_name,
            url_4_metric_name
]


denom_name = url_metric_name_list[which_url_determines_denominator - 1]
print(denom_name)

# @title Query Dolphin
def get_experiment_data(id_type):

  if id_type == "visitor id":
    id = 'visitor_id'
    table = '`nbcu-ds-prod-001.PeacockDataMartProductAnalyticsSilver.silver_commerce_web_test_visitor_level`'
  elif id_type == "tracking id":
    id = 'tracking_id'
    table = '`nbcu-ds-prod-001.PeacockDataMartProductAnalyticsSilver.silver_commerce_web_test_tracking_id_level`'
  else:
    print("Invalid user type parameter given. Reconfigure Experiment Parameters please.")
    pass

  query = f"""
    SELECT
    {id} AS id,
    min_date AS exposure_date,
    test_name,
    variant_name,
    status,
    visitor_entitlement,
    billing_cycle,
    user_status,
    been_to_url1,
    been_to_url2,
    been_to_url3,
    been_to_url4
    FROM
    {table}
    WHERE 1=1
    AND test_id = '{experiment_id}'
    AND min_date BETWEEN '{experiment_start_date}' AND '{experiment_end_date}'
  """

  df  = pd.read_gbq(query,project_id = bq_config_vars['project_id'], use_bqstorage_api=True, progress_bar_type = 'tqdm')

  experiment_name = df['test_name'].unique().astype('str')[0]
  """
  # Specify the file path where you want to save the text file
  file_path = f"query.txt"  # You can change the file name and path as needed
  # Open the file in write mode and write the string to it
  with open(file_path, "w") as text_file:
      text_file.write(query)
  # Confirm that the file has been saved
  print(f"BQ Query has been saved to '{file_path}'.")
  """
  return(df, experiment_name,query)

results = get_experiment_data(id_type)
query_df, experiment_name, query = results[0], results[1], results[2]

#@title Set up Output Directory
# Create subfolders
al_name, htv_name, tsv_name = "Analysis Log", "Hypothesis Test Visuals", "Time Series Visuals"
parent_folder = "Experiment Analysis Outputs: " + experiment_name
# Create the parent folder if it doesn't exist
if not os.path.exists(parent_folder):
    os.mkdir(parent_folder)
  # Change the working directory to the parent folder
os.chdir(parent_folder)
# Create the subfolders if they don't exist
for subfolder in [al_name, htv_name, tsv_name]:
    if not os.path.exists(subfolder):
        os.mkdir(subfolder)

# Switch working directory back to the parent folder
os.chdir("..")  # Use ".." to navigate up one level
# Verify the current working directory
current_directory = os.getcwd()
print("Current working directory:", current_directory)

# @title Parse out Conversions
def parse_conversions(df):
  """
  update been_to_url columns to incorporate "AND" conditioning/ account for data spillovers
  """
  url4_condition = ((df['been_to_url1'] == 1) & (df['been_to_url2'] == 1) & (df['been_to_url3'] == 1) & (df['been_to_url4'] == 1))
  url3_condition = (url4_condition | ((df['been_to_url1'] == 1) & (df['been_to_url2'] == 1) & (df['been_to_url3'] == 1) & (df['been_to_url4'] == 0)))
  url2_condition = (url3_condition | ((df['been_to_url1'] == 1) & (df['been_to_url2'] == 1) & (df['been_to_url3'] == 0) & (df['been_to_url4'] == 0)))
  url1_condition = (url2_condition | ((df['been_to_url1'] == 1) & (df['been_to_url2'] == 0) & (df['been_to_url3'] == 0) & (df['been_to_url4'] == 0)))

  df[url_1_metric_name] = url1_condition
  df[url_2_metric_name] = url2_condition
  df[url_3_metric_name] = url3_condition
  df[url_4_metric_name] = url4_condition

  conversion_cols = [df[url_1_metric_name], df[url_2_metric_name], df[url_3_metric_name], df[url_4_metric_name]]
  for i, v in enumerate(conversion_cols):
    if i+1 == which_url_determines_conversion:
      conversion_col = v

  df['Monthly Conversions'] = (conversion_col) & (df['billing_cycle'] == 'MONTHLY')
  df['Annual Conversions'] = (conversion_col) & (df['billing_cycle'] == 'ANNUAL')
  df['Premium Conversions'] = (conversion_col) & (df['visitor_entitlement'] == 'Premium')
  df['Premium Plus Conversions'] = (conversion_col) & (df['visitor_entitlement'] == 'Premium+')

  df['Premium Monthly Conversions'] = df['Premium Conversions'] & (df['billing_cycle'] == 'MONTHLY')
  df['Premium Monthly Plus Conversions'] = df['Premium Plus Conversions'] & (df['billing_cycle'] == 'MONTHLY')
  df['Premium Annual Conversions'] = df['Premium Conversions'] & (df['billing_cycle'] == 'ANNUAL')
  df['Premium Plus Annual Conversions'] = df['Premium Plus Conversions'] & (df['billing_cycle'] == 'ANNUAL')
  return(df)

conversion_df = parse_conversions(query_df)
display(conversion_df.head(20))
display(conversion_df.describe())
display(conversion_df.info())

# @title Aggregate Counts
def aggregate_counts(df):
  """
  Returns: dictionary of dataframes with sum/cumsum of raw, granular conversions (defined in 'value_list')
  """
  #value list will be all booleans from above
  value_list = df.select_dtypes(include='bool').columns.tolist()
  try:
    value_list.remove('None')
    print(value_list)
  except:
    print("No booleans for value list found")
  try:
    date_col = df.select_dtypes(include = 'dbdate').columns.tolist()[0]
  except:
    print("No Date column Found Error!")

  try: #agg to grain level
    grain_level_df = df.groupby([date_col, "variant_name", "user_status"])[value_list].agg('sum')
    # get cumulative daily sum by grain
    cum_daily_grain_level_df = grain_level_df.sort_values(["variant_name", "user_status",date_col]).groupby(['variant_name', 'user_status']).cumsum()
  except:
    grain_level_df = pd.DataFrame()
    cum_daily_grain_level_df = pd.DataFrame()
    print("daily grain level aggregation failed")

  try: # agg to variant level
    daily_variant_level_df = df.groupby([date_col, 'variant_name'])[value_list].agg('sum')
    # get cumulative daily sum by variant
    cum_daily_variant_level_df = daily_variant_level_df.sort_values(["variant_name", date_col]).groupby('variant_name').cumsum()
  except:
    daily_variant_level_df = pd.DataFrame()
    cum_daily_variant_level_df = pd.DataFrame()
    print('daily variant level agg failed')

  try:  #agg to overalls
    overall_grain_level_df = df.groupby(['variant_name', 'user_status'])[value_list].agg('sum')
    # #aggregate to overall by variant
    overall_variant_level_df = df.groupby(['variant_name'])[value_list].agg('sum')
  except:
    overall_grain_level_df = pd.DataFrame()
    overall_variant_level_df = pd.DataFrame()
    print('overall level agg failed')

  #place dataframes into a dictionary to return
  analysis_data_dict = {
      'Daily Variant-User Status Level' : grain_level_df.reset_index(),
      'Daily Variant Level': daily_variant_level_df.reset_index(),
      'Cumulative Daily Variant-User Status Level': cum_daily_grain_level_df.reset_index(),
      'Cumulative Daily Variant Level': cum_daily_variant_level_df.reset_index(),
      'Overall Variant-User Status Level': overall_grain_level_df.reset_index(),
      'Overall Variant Level': overall_variant_level_df.reset_index()
  }
  return(analysis_data_dict)

#instantiate dictionary of aggregated data
agg_dfs_dict = aggregate_counts(conversion_df)
for i,v in agg_dfs_dict.items():
  display(f"Info on {i}")
  display(v.head())
  display(v.describe())
  display(v.info())

#@title Compute Conversion Rate
def compute_conversions(dict):
  """ Arg: a dictionary of dataframes
  Returns: updated dictionary of dataframes with conversion metrics as new columns for each dataframe
  """
  dict_copy = dict.copy()
  for key, val in enumerate(dict_copy):
    #grab a data frame
    df = dict_copy[val]
    #get numeric numerator columns, exclude denominator column
    num_cols = df.select_dtypes(include = 'int64').columns.tolist()
    num_cols.remove(denom_name) # dont need conversion on denominator
    #compute conversion rate for each numerator/denominator
    for i in num_cols:
      if i.endswith("s") or i.endswith("es"):
        new_i = i[:-1]
        print(new_i)
      else:
        new_i = i
      new_col = new_i+" Rate"
      df[new_col] = df[i]/df[denom_name]
    #update dictionary
    updates = {val: df}
    dict_copy.update(updates)
  return(dict_copy)
#create conversion dict
agg_conversions_dict = compute_conversions(agg_dfs_dict)
#display(agg_conversions_dict)

for i,v in agg_conversions_dict.items():
  print(i)
  print(v.dtypes)
  print('\n')
  print("Describing: ",i,  v.head())

# @title Compute Relative Lifts
def compute_relative_lift(dict):
  dict_copy = dict.copy()
  for key, val in enumerate(dict_copy):
    df = dict_copy[val]
    control_df = df[df['variant_name'] == control_name]
    join_keys = df.select_dtypes(exclude=np.number).columns.tolist()
    join_keys.remove('variant_name')
    #get rate columns
    rate_cols =  [col for col in df.columns if np.logical_or('Rate' in col, col== denom_name)]
    control_suffix = '_control_val'
    #merge in corresponding control value based on join keys
    if len(join_keys) > 0:
      df_with_control = df.merge(control_df, on = join_keys, how = 'left', suffixes = ('', control_suffix))
    else:
      df['key'] =1
      control_df['key'] = 1
      df_with_control = df.merge(control_df, on = 'key', how = 'left', suffixes = ('', control_suffix))
      df_with_control.drop(columns = 'key', inplace = True)
    # print(df_with_control.columns)
    for i in rate_cols:
      new_col = i+" Relative Lift vs Control"
      # add new column which is current value/ control value
      df_with_control[new_col] = df_with_control[i]/df_with_control[i+control_suffix] - 1.0
    # parse joined control values after calculation
    columns_to_drop = [col for col in df_with_control.columns if control_suffix in col]
    # Drop the columns containing control suffix, no longer needed
    df_with_control.drop(columns=columns_to_drop, inplace=True)
    #update dictionary
    updates = {val: df_with_control}
    dict_copy.update(updates)
  return(dict_copy)
#add relative lifts to dictionary
agg_lifts_dict = compute_relative_lift(agg_conversions_dict)
for i,v in agg_lifts_dict.items():
  display(i, v.columns)

# @title Visualize Time Series Data
def visualize_experiment_data(data_dict):
  for k, v in data_dict.items():
      # print(v.columns)
      # print(v['key'].unique())
      # print(v['find_key'].unique())
    if k.find('Daily Variant Level') >= 0:
      # Define metric groups based on user inputs
      metric_list_ov = url_metric_name_list.copy()
      try:
        metric_list_ov.remove("None") #remove Nones
      except ValueError:
        pass

      #get copy to edit
      metric_list_ov_fun = metric_list_ov.copy()
      #drop denominator column based on user inputs
      metric_list_ov_fun.remove(denom_name)

      """ NOTE: The following hard-coded lines"""
      metric_list_sub = ['Monthly Conversions', 'Annual Conversions', 'Premium Conversions', 'Premium Plus Conversions']
      #drop s and match metric naming conventions
      metric_list_ov_fun_new = [x[:-1] for x in metric_list_ov_fun if x.endswith('s')]
      metric_list_sub_new = [x[:-1] for x in metric_list_sub if x.endswith('s')]
      metric_list_ov_rate = [m + " Rate" for m in metric_list_ov_fun_new]
      metric_list_sub_rate = [m + " Rate" for m in metric_list_sub_new]
      metric_list_ov_vs_control = [m + " Relative Lift vs Control" for m in metric_list_ov_rate]
      metric_list_sub_vs_control = [m + " Relative Lift vs Control" for m in metric_list_sub_rate]

      metric_lol = [
            metric_list_ov, metric_list_sub,
            metric_list_ov_rate, metric_list_sub_rate,
            metric_list_ov_vs_control, metric_list_sub_vs_control
      ]
      metric_names_lol = ['Funnel Metrics','Subscription Metrics','Funnel Rates',
                          'Subscription Rates','Funnel Rates vs Control','Subscription Rates vs Control']
      """ End of hard-coding heavy section """
      for i, metric_group in enumerate(metric_lol):
        print(i, metric_group, metric_names_lol[i])
        chart_name = f"{metric_names_lol[i]}: {k}.png"
        print(chart_name)
        # Create a single set of subplots for each metric group
        fig, axes = plt.subplots(len(metric_group), 1, figsize=(8, len(metric_group)*4), sharey = False)
        plt.style.use('seaborn-v0_8-darkgrid')
        palette = sns.color_palette("colorblind")
        for a, metric in enumerate(metric_group):
          ax = axes[a]
          for i, variant_name in enumerate(v['variant_name'].unique()):
            sub_v = v[v['variant_name'] == variant_name]
            color = palette[i % len(palette)]
            if variant_name == control_name and metric.find('Relative Lift') > 0 :
              pass
            else: # if relative_lift_vs_control visualization and control variant, don't plot the data
              if metric.find('Relative Lift') > 0:
                ax.axhline(y=0.0, color='red', linestyle='dotted', alpha = .5)
              ax.plot(sub_v['exposure_date'], sub_v[metric], marker='o', markersize=3, linestyle='-', label=variant_name, color=color)
              # set up y-axis start point
              ylim_bot = min(sub_v[sub_v['variant_name'] == variant_name][metric].min() * 1.5, 0)
              ylim_top = max(sub_v[sub_v['variant_name'] == variant_name][metric].max() * 1.5, 0)
              ax.set_ylim(ylim_bot , ylim_top, auto = True)
          ax.set_xlabel('Exposure Date', fontsize = 8)
          ax.set_xticklabels(ax.get_xticklabels(), rotation=45, fontsize = 9)
          ax.set_title(f'{k}: {metric}')
          ax.legend(loc='best',  frameon=True, fontsize=8)
        plt.tight_layout(pad = 2)
        plt.show()
        print(chart_name)
        fig.savefig(parent_folder + '/' + tsv_name +'/'+  chart_name)
        plt.clf()
    else:
        pass

visualize_experiment_data(agg_lifts_dict)

# @title Conduct Hypothesis Testing
#return dictionary of tables with hypothesis testing results
def hypothesis_test(dict, alpha):
#new_dict to hold new dictionary of keys where values = dataframe with hypothesis test result s
  new_dict = {}
# iterate through dictionary
  for k,v in dict.items():
    # ADD KEYS TO dataframes
    join_keys_list = v.select_dtypes(include='object').columns.tolist()
    print("join_keys", join_keys_list)
    # Join the selected columns across rows and store the result in a new column
    v['key'] = v[join_keys_list].apply(lambda x: '-'.join(x), axis=1)
    #grab non variant_name cols to create a key to grab corresponding control
    find_keys_list = (join_keys_list.copy())
    find_keys_list.remove('variant_name')

    if len(find_keys_list) == 0: #set a key if none
      v['find_key'] = 'join'
    else: #create a concatenated column of non-numerics with variant_name col removed
      v['find_key'] = v[find_keys_list].apply(lambda x: '-'.join(x), axis=1)

    print("find_keys", find_keys_list)
    #determine if dataframe is eligible for hypothesis testing
    if k.find('Overall') == 0: #conduct hypothesis testing on "overall_" level data
      print("Starting hypothesis testing for: ",k)
      #create parsing key to get each unique grain-variant

      #split into control vs variants
      control_df = v[v['variant_name'] == control_name]
      variants_df = v[v['variant_name'] != control_name]

      print(control_df.head())

      # list of metrics to hypothesis test, int64 parses to non-normalized count metrics
      metric_list = v.select_dtypes(include='int64').columns.tolist()
      metric_list.remove(denom_name)
      print("metric_list to conduct hyp tests on ", metric_list)

      # instantiate a dataframe to store hypothesis test results
      hyp_df = pd.DataFrame(columns = ['variant_group', 'metric', 'sample_n', 'control_n', 'sample_mean', 'control_mean', 'absolute_diff', 'relative_diff', \
                                       'standard_error', 'p_value', 'alpha','ci_lower_absolute', 'ci_upper_absolute','ci_lower_relative', 'ci_upper_relative'])

      for vk in variants_df.key.unique(): #iterate through each unique subsetted variant, depends on key uniqueness to work correctly!
        vdf = variants_df[variants_df['key'] == vk] #subset variants_df
        fk = vdf['find_key'].iloc[0] # get find key of variant
        cdf = control_df[control_df['find_key'] == fk] # grab correct control row
        print("cdf: ", cdf.head(10))
        for m in metric_list: #iterate through each metric
          # get control and variant counts of metric
          c_count = cdf[m].iloc[0]
          v_count = vdf[m].iloc[0]

          # get control and variant denominator
          c_nob = cdf[denom_name].iloc[0]
          v_nob = vdf[denom_name].iloc[0]

          prop_c = c_count/c_nob
          prop_v = v_count/v_nob
          var = prop_c * (1 - prop_c)/c_nob + prop_v * (1 - prop_v)/v_nob
          se = np.sqrt(var)

          z_crit = stats.norm().ppf(1 - alpha/2)

          # put into 2x2 array
          counts = np.array([v_count, c_count])
          nobs = np.array([v_nob, c_nob])
          stat, pval = prop.proportions_ztest(counts, nobs)
          sample_mean = prop_v
          control_mean = prop_c
          sample_diff_abs = prop_v - prop_c
          sample_diff_relative = prop_v/prop_c - 1
          ci_lower_abs = sample_diff_abs - z_crit * se
          ci_upper_abs = sample_diff_abs + z_crit * se
          ci_lower_rel = ci_lower_abs/control_mean
          ci_upper_rel = ci_upper_abs/control_mean

          hyp_test_row = [vk, m, v_nob, c_nob, sample_mean, control_mean, sample_diff_abs, sample_diff_relative, \
                          se, pval, alpha, ci_lower_abs, ci_upper_abs, ci_lower_rel, ci_upper_rel]
          hyp_df.loc[len(hyp_df)] = hyp_test_row

      #drop s, add _rate to metric names to communicate we conducted proportions test
      hyp_df['metric'] = [x[:-1] for x in hyp_df['metric'] if x.endswith('s')]
      hyp_df['metric'] = hyp_df['metric'] + ' Rate'
      updates = {k: hyp_df}
      display(k, hyp_df)
      new_dict.update(updates)
    else: # dataframe not eligible for hyp testing
      pass
  return(new_dict)

hypothesis_dict = hypothesis_test(agg_lifts_dict, significance_level)

# @title Visualize Hypothesis Testing Data
def visualize_hyp_data(dict):
  """
  cycle through dictionary of hyp data frames
  plot p-values for each metric with significance line
  plot relative lift confidence intervals for each metric
  Note: each metric in hyp dict is expressed as a rate (normalized by visitors)
  """
  for k,v in dict.items():
    print("Visualizing", k)
    group_list = v['variant_group'].unique().tolist()
    #set up subplots
    plt.style.use('seaborn-muted')
    palette = sns.color_palette("colorblind")
    for id, g in enumerate(group_list):
      plt.figure(figsize = (8,8))
      #subset data frame to just 1 group
      var_v = v[v['variant_group'] == g]
      metrics_list = ['Click Rate', 'Conversion Rate', 'Monthly Conversion Rate', 'Annual Conversion Rate',
                      'Premium Conversion Rate', 'Premium Plus Conversion Rate']
      sub_v = var_v[var_v['metric'].isin(metrics_list)]
      #plot, name and save p-values
      plt.bar(sub_v['metric'], sub_v['p_value'], color = palette[0])
      print(sub_v['metric'])
      #plot data labels
      for i, val in enumerate(sub_v['p_value']):
        plt.text(i, val, round(val,3), ha = 'center', fontsize = 9)
      plt.xticks(fontsize=10, rotation=60)
      plt.yticks(fontsize=10)
      plt.ylabel("p-value", size=10)
      plt.title("P-values for "+ experiment_name + ': ' + g, size=12)
      plt.axhline(y=0.1, color='red', linestyle='dotted', label='Threshold (0.1)', alpha = .5)  # Add the dotted line
      plt.tight_layout(pad = 2)
      plt.savefig(f"{parent_folder}/{htv_name}/{k}; {g}: P-values.png")
      plt.show()
      plt.clf()

      #plot name and save confidence interval
      plt.figure(figsize = (8,8))
      plt.style.use('seaborn-muted')

      def percentage_formatter(x, pos):
        return f'{x * 100:.0f}%'
      for x, lower,relative, upper in zip(range(len(sub_v)), sub_v['ci_lower_relative'],sub_v['relative_diff'], sub_v['ci_upper_relative']):
        plt.plot((x,x,x),(lower,relative,upper), 'o-', color = palette[0] ,markersize = 2)
        plt.text(x+.05, lower, round(lower,3), ha = 'left', fontsize = 7.5)
        plt.text(x+.05, relative, round(relative,3), ha = 'left', fontsize = 7.5)
        plt.text(x+.05, upper, round(upper,3), ha = 'left', fontsize = 7.5)
        plt.xticks(range(len(sub_v)),list(sub_v['metric']), rotation = 60, fontsize=10)


        plt.gca().yaxis.set_major_formatter(FuncFormatter(percentage_formatter))
        plt.ylabel("Relative Lift", size=10)
        plt.title(str(percentage_formatter(1- significance_level,0))+ " Confidence Intervals for "+ experiment_name + ': ' + g, size=12)
      plt.axhline(y=0.0, color='red', linestyle='dotted', alpha = .5)
      plt.tight_layout(pad = 2)
      plt.savefig(f"{parent_folder}/{htv_name}/{k}: {g}: Relative Lift CI.png")
      plt.show()
      plt.clf()
  plt.close()
  pass
#run function
visualize_hyp_data(hypothesis_dict)

# @title Log Data into Excel Sheets
def log_data(dict, name, parent_folder, subfolder_name):
  """ Take in a dictionary of dataframes and import into xlsx spreadsheet
  Log down all data used for visualizations and hypothesis testing
  """
  # Specify the Excel file name
  excel_file = parent_folder+ '/' + subfolder_name + '/'+ name + '.xlsx'

  # Create a Pandas Excel writer using ExcelWriter
  with pd.ExcelWriter(excel_file) as writer:
      # Iterate through the dictionary and write each DataFrame to a sheet
      for sheet_name, df in dict.items():
          df.to_excel(writer, sheet_name=sheet_name, index=False)

log_data(agg_lifts_dict, 'Analysis Data', parent_folder, al_name )
log_data(hypothesis_dict, 'Hypothesis Testing Data', parent_folder, al_name)



